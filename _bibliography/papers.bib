---
---

@article{zhang2023fooling,
  abbr      = {Sensors},
  bibtex_show  = {true},
  title     = {Fooling Examples: Another Intriguing Property of Neural Networks},
  author    = {Zhang, Ming and Chen, Yongkang and Qian, Cheng},
  journal   = {Sensors},
  volume    = {23},
  number    = {14},
  pages     = {6378},
  year      = {2023},
  publisher = {MDPI}
}

@article{chen2022dynamic,
  abbr         = {TrustCom},
  bibtex_show  = {true},
  title        = {Dynamic and Diverse Transformations for Defending Against Adversarial Examples},
  author       = {Chen, Yongkang and Zhang, Ming and Li, Jin and Kuang, Xiaohui and Zhang, Xuhong and Zhang, Han},
  journal      = {2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)},
  abstract     = {It is demonstrated that deep neural networks can be easily fooled by adversarial examples. To improve the robustness of neural networks against adversarial attacks, substantial research on adversarial defenses is being carried out, of which input transformation is a typical category of defenses. However, because the transformation also has an impact on the accuracy of clean examples, the existing transformation-based defenses usually adopt minor transformations such as shift and scaling, which limits the defense effect of the transformation to some extent. To this end, we propose a method by using dynamic and diverse transformations for defending against adversarial attacks. Firstly, we constructed a transformation pool that contains both minor and major transformations (e.g., flip, rotate). Secondly, we retrained the model with the data transformed by major transformations to ensure that the performance of model itself is not affected. Finally, we dynamically select transformations to preprocess the input of the model to defend against adversarial examples. We conducted extensive experiments on MNIST and CIFAR-10 datasets and compared our method with the state-of-the-art adversarial training and transformation-based defenses. The experimental results show that our proposed method outperforms the existing methods, improving the robustness of the model against adversarial examples greatly while maintaining high accuracy on clean examples. Our code is available at https://github.com/byerose/DynamicDiverseTransformations.},
  html         = {https://www.computer.org/csdl/proceedings-article/trustcom/2022/942500a976/1LFM0C926Sk},
  pages        = {976--983},
  year         = {2022},
  organization = {IEEE}
}

@article{chen2022adversarial,
  abbr         = {ICIVC},
  bibtex_show  = {true},
  title        = {Adversarial attacks and defenses in image classification: A practical perspective},
  author       = {Chen, Yongkang and Zhang, Ming and Li, Jin and Kuang, Xiaohui},
  journal      = {2022 7th International Conference on Image, Vision and Computing (ICIVC)},
  abstract     = {The rapid and steady development of machine learning, especially deep learning, has promoted significant progress in the field of image classification. However, Machine learning models are demonstrated to be vulnerable to adversarial examples, which pose serious threats in security-critical applications. This paper summarizes the adversarial attacks and defenses from a practical perspective, facing the field of image classification. We further analyze and evaluate the characteristics and performance of various defense techniques from four aspects: gradient masking, adversarial training, adversarial examples detection and input transformations. We discuss the advantages and disadvantages of different defenses. Finally, the future development trend of defense techniques against adversarial examples is discussed. We hope our study will advance the research in machine learning security.},
  html         = {https://ieeexplore.ieee.org/document/9886997},
  pages        = {424--430},
  year         = {2022},
  organization = {IEEE}
}

@book{przibram1967letters,
  bibtex_show = {true},
  title       = {Letters on wave mechanics},
  author      = {Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year        = {1967},
  publisher   = {Vision},
  preview     = {wave-mechanics.gif}
}

@article{PhysRev.47.777,
  abbr       = {PhysRev},
  title      = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author     = {Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract   = {In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal    = {Phys. Rev.,},
  volume     = {47},
  issue      = {10},
  pages      = {777--780},
  numpages   = {0},
  year       = {1935},
  month      = {May},
  doi        = {10.1103/PhysRev.47.777},
  url        = {http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html       = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf        = {example_pdf.pdf},
  altmetric  = {248277},
  dimensions = {true},
  selected   = {true}
}